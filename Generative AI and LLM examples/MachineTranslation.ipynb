{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406927f1-a275-4497-967a-99b062e7c2c4",
   "metadata": {},
   "source": [
    "# Machine Translation with Sequence-to-Sequence and RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371ae8d9-123e-4c23-92fa-8181dd7ff4fd",
   "metadata": {},
   "source": [
    "This lab will demonstrate fundamentals of Sequence-to-sequence RNN model for translation.  Using various libraries: `pandas`, `numpy`, `sklearn`, `seaborn`, `matplotlib`, `torch`, `spacy`.  (NOTE: we will use `spacy` for the natural language processing (NLP). \n",
    "\n",
    "Will demonstrate use of the Multi30K dataset (a large machine translation dataset with extensive English to German sentence pairs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca3315ba-c1af-4634-a00d-ae0496aa69cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.8.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting de-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from de-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (58.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2025.8.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.3.1)\n",
      "Requirement already satisfied: wrapt in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Installation complete! Testing imports...\n",
      "✓ NumPy 1.26.4\n",
      "✓ PyTorch 2.2.2\n",
      "✓ spaCy 3.7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "# Install packages\n",
    "os.system(f\"{sys.executable} -m pip install -qq 'numpy<2.0' 2>/dev/null\")\n",
    "os.system(f\"{sys.executable} -m pip install -qq torch==2.2.2 torchvision==0.17.2 torchtext==0.17.2 2>/dev/null\")\n",
    "os.system(f\"{sys.executable} -m pip install -qq 'spacy<3.8' 'thinc<8.3' 2>/dev/null\")\n",
    "os.system(f\"{sys.executable} -m pip install -qq pandas matplotlib seaborn scikit-learn portalocker 2>/dev/null\")\n",
    "os.system(f\"{sys.executable} -m pip install -qq torchdata==0.7.1 nltk 2>/dev/null\")\n",
    "os.system(f\"{sys.executable} -m spacy download en_core_web_sm 2>/dev/null\")\n",
    "\n",
    "os.system(f\"{sys.executable} -m spacy download de_core_news_sm 2>/dev/null\")\n",
    "\n",
    "# Verify\n",
    "print(\"Installation complete! Testing imports...\")\n",
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "print(f\"✓ NumPy {np.__version__}\")\n",
    "print(f\"✓ PyTorch {torch.__version__}\")\n",
    "print(f\"✓ spaCy {spacy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2119cd4-3cd3-4191-af8d-543683b8769b",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d81050a-2828-4b2f-88a5-ba8962677e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer #import get_tokenizer function from torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator #import function that builds vocabulary from tokenized text\n",
    "from nltk.translate.bleu_score import sentence_bleu #Used for evals/BLEU score metrics\n",
    "from torchtext.datasets import multi30k, Multi30k #Import the utilities/functions to access Multi30k dataset\n",
    "from typing import Iterable, List # For type hints\n",
    "from torch.nn.utils.rnn import pad_sequence #for batch sequence length noramlization\n",
    "from torch.utils.data import DataLoader #for creation of data batches\n",
    "from torchdata.datapipes.iter import IterableWrapper, Mapper\n",
    "import torchtext #Torchtext for NLP tasks\n",
    "\n",
    "import torch #Main PyTorch library for tensor operations\n",
    "import torch.nn as nn #utilities for building blocks of the neural network (e.g. layers, loss functions)\n",
    "import torch.optim as optim #optimization algo for model weight updates during training\n",
    "\n",
    "#suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4da97be-2d47-4e30-9982-a0ba1ab210fe",
   "metadata": {},
   "source": [
    "# Sequence to Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1487082-9cd3-433d-8c1a-c988fdff1c29",
   "metadata": {},
   "source": [
    "We will implement a Seq2Seq model in PyTorch.  Sequence-to-sequence or `seq2seq` models useful for: \n",
    "- Translation from source language to target language\n",
    "- Chat bots (e.g. answering question or generatugin natural language responses to input sequences)\n",
    "- Summarization\n",
    "\n",
    "## Sequence-to-sequence (seq2seq) architecture\n",
    "- Seq2seq utilize an encoder-decoder structure: (1) the encoder encodes input seq into fixed-dimensional representation context vector (ht) (2) the decoder generates output sequence based on encoded context vector.\n",
    "- This architecture takes input token x_t and embedding layer converts word ID to dense vector. RNN decoder takes the embedded word and previous hidden state (h_t) to output new hidden state h_{t+1}. A Linear layer projects the new hidden state to vocab size and outputs probability distribution for next word prediction. This process repeats, with each predicted word becoming the next input x_t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b32b8818-b0af-4411-934e-8382017fc2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the encoder class\n",
    "\n",
    "class Encoder(nn.Module): #Encoder class inherits from PyTorch nn.Module base class\n",
    "    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n",
    "        super().__init__() #run initialization code from parent class\n",
    "\n",
    "        self.hid_dim = hid_dim # Save hidden size (for decoder)\n",
    "        self.n_layers = n_layers # Save layer count (for decoder)\n",
    "\n",
    "        #Create the three layers\n",
    "        self.embedding = nn.Embedding(vocab_len, emb_dim) #Word to vec lookup\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, input_batch): #forward runs encoding to convert input sentences into context vectors\n",
    "        embed = self.dropout(self.embedding(input_batch)) #Store word vectors in embed + dropout\n",
    "        #embed = embed.to(device) #Move to device for computation\n",
    "        outputs, (hidden, cell) = self.lstm(embed) # Pass embeddings through LSTM, get outputs and states\n",
    "        return hidden, cell #return encoded representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd506a6b-0e8b-4fa2-82f1-6f57a7ad7fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input(src) tensor: torch.Size([5, 1])\n",
      "Hidden tensor from encoder: tensor([[[ 0.1518, -0.0946, -0.3460, -0.2502, -0.0561, -0.0278, -0.1989,\n",
      "           0.1697]]], grad_fn=<StackBackward0>) \n",
      "Cell tensor from encoder: tensor([[[ 0.2645, -0.2014, -0.6276, -0.5840, -0.0856, -0.0964, -0.3747,\n",
      "           0.3822]]], grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Optional but recommended: Test encoder and verify functionality\n",
    "vocab_len = 8\n",
    "emb_dim = 10\n",
    "hid_dim=8\n",
    "n_layers=1\n",
    "dropout_prob=0.5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)\n",
    "\n",
    "src_batch = torch.tensor([[0,3,4,2,1]])\n",
    "# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n",
    "src_batch = src_batch.t().to(device)\n",
    "print(\"Shape of input(src) tensor:\", src_batch.shape)\n",
    "hidden_t , cell_t = encoder_t(src_batch)\n",
    "print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2947bb1b-9326-4f21-b9a5-60c8a2b99abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The decoder generates target language words one by one by taking the previous word and encoder context, \n",
    "processing them through the embedding layer/LTSM and outputting probability scores across the entire\n",
    "target set vocabulary.  \n",
    "\n",
    "Each forward pass produces one word prediction along with updated states for the next step, allowing the\n",
    "decoder to build translations word by word while maintaining context from both the source sentence and previously\n",
    "generated words.\n",
    "'''\n",
    "\n",
    "#Implement decoder class that inherits from nn.Module\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__() \n",
    "\n",
    "        self.output_dim = output_dim #Store target vocab size (output_dim is vocab size)\n",
    "        self.hid_dim = hid_dim #Store hidden dimension\n",
    "        self.n_layers = n_layers #Store num of LTSM layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim) # Create learnable lookup table for target word ID → vector conversion\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout) # Create LSTM decoder with input size emb_dim\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim) # Linear layer to project hidden states to vocab size\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # Convert logits to log probabilities for word prediction\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer for regularization during training\n",
    "\n",
    "    # Forward method processes one word at a time through decoder, generating next word prediction\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input=[batch_size], hidden/cell=[n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        # Add sequence dimension for LSTM compatibility\n",
    "        input = input.unsqueeze(0)  # [1, batch_size]\n",
    "        \n",
    "        # Convert word IDs to vectors and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input))  # [1, batch_size, emb_dim]\n",
    "        \n",
    "        # Process through LSTM with previous states\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # output=[1, batch_size, hid_dim], hidden/cell=[n_layers, batch_size, hid_dim]\n",
    "        \n",
    "        # Remove seq dimension and project to vocabulary size\n",
    "        prediction_logit = self.fc_out(output.squeeze(0))  # [batch_size, output_dim]\n",
    "        \n",
    "        # Convert logits to log probabilities\n",
    "        prediction = self.softmax(prediction_logit)  # [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e46c0ee-5c83-4f1c-bb95-103f237cd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test decoder by creating an instance \n",
    "output_dim = 6\n",
    "emb_dim=10\n",
    "hid_dim = 8\n",
    "n_layers=1\n",
    "dropout=0.5\n",
    "decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115256aa-0b49-4472-9333-9779229ef417",
   "metadata": {},
   "source": [
    "Next we will create a training loop that connects encoder output to decoder input.  The encoder runs once (to process the entire input sequence) to create the summary (hidden and cell states), then the decoder runs multiple times (once per output word to iteratively generates the output sequence word by word) to build the translation word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ff9ea59-c9cd-4821-b46c-c56a4564699c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.8125, -2.1189, -1.8993, -1.4792, -2.1225, -1.5171]],\n",
      "\n",
      "        [[-1.7986, -2.0820, -1.9791, -1.4562, -1.9319, -1.6420]],\n",
      "\n",
      "        [[-1.7508, -2.1017, -1.9372, -1.4581, -2.0165, -1.6388]],\n",
      "\n",
      "        [[-1.7903, -2.0690, -1.9052, -1.5341, -1.9829, -1.5865]]],\n",
      "       device='mps:0', grad_fn=<CopySlices>) torch.Size([5, 1, 6])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Create the encoder-decoder connection to allow model to process (src, trg) pairs to generate the translation\n",
    "#Implement the decoder training loop that generates translations step by step\n",
    "\n",
    "# Setup for decoder training loop\n",
    "teacher_forcing_ratio = 0.5  # During training: balances learning from correct answers vs own predictions\n",
    "trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)  #Target sequence (test example, real examples will come from Multi30K dataset)\n",
    "batch_size = trg.shape[1] #save batch dimension to batch_size\n",
    "trg_len = trg.shape[0] #Save seq length \n",
    "trg_vocab_size = decoder_t.output_dim #Get target vocab size\n",
    "\n",
    "# Initialize storage for all predictions; outputs_t tensor collects decoder's probability distributions for what model thinks each word should be at every step of generation \n",
    "outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "\n",
    "# Move encoder outputs to device\n",
    "hidden_t = hidden_t.to(device)\n",
    "cell_t = cell_t.to(device)\n",
    "\n",
    "# Start with <bos> token; extracts first row position 0 from target sequence\n",
    "input = trg[0,:] #extract row 0 , all columns\n",
    "\n",
    "# Generate one word at a time - the for loop runs decoder (Takes input token + states → produces word probabilities + updated states) , stores the productions in output_t, \n",
    "for t in range(1, trg_len):\n",
    "    # Run decoder\n",
    "    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n",
    "    outputs_t[t] = output_t\n",
    "    \n",
    "    # Teacher forcing: use true target or prediction\n",
    "    teacher_force = random.random() < teacher_forcing_ratio\n",
    "    top1 = output_t.argmax(1)\n",
    "    input = trg[t] if teacher_force else top1\n",
    "\n",
    "print(outputs_t, outputs_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7c22d65-a6b2-4eab-b295-750860c2f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trg = \n",
      "tensor([[0],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3],\n",
      "        [3]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#Extract actual predicted word IDs from probability distributions\n",
    "print(\"trg = \")\n",
    "pred_tokens = outputs_t.argmax(2)\n",
    "print(pred_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf76b24-6a0f-4cb8-a387-4453d1f8b621",
   "metadata": {},
   "source": [
    "# Implementing Seq2Seq model in PyTorch (Defining the model)\n",
    "\n",
    "We will connect the encoder and decoder components to create the seq2seq model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ced609ab-e11c-4ba1-9bfe-73eb5f6fbf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq class combines encoder and decoder into complete translation model\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, trg_vocab): #parameters needed for the translation process\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder #store encoder instance\n",
    "        self.decoder = decoder #store decoder instance\n",
    "        self.device = device  \n",
    "        self.trg_vocab = trg_vocab #store target vocabulary (for special tokens)\n",
    "\n",
    "        #Ensure encoder and decoder compatibility\n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "\n",
    "    #Forward method orchestrates the translation process\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        # Extract dimensions from the target tensor\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # Initialize tensor to collect all decoder outputs\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode entire source sequence to get context vecs\n",
    "        # last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "\n",
    "        #Start decoding with <bos> token\n",
    "        input = trg[0,:]\n",
    "\n",
    "        # Generate target sequence one token at a time\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926cdf70-b6e2-49c6-afdf-ba219dc38927",
   "metadata": {},
   "source": [
    "# PyTorch Model Training (Training the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d700001d-8686-4d1a-9724-80fd1cd43baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function that runs one epoch of training on the seq2seq model\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()  # Set model to training mode (enables dropout)\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # Create progress bar wrapper\n",
    "    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "    \n",
    "    # Process each batch of source-target sentence pairs\n",
    "    for i, (src, trg) in enumerate(train_iterator):  # FIXED: Use train_iterator not iterator\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients from previous step\n",
    "        \n",
    "        # Forward pass: encoder processes src, decoder generates predictions for trg\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Reshape for loss calculation: skip <sos> token at position 0\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)  # Flatten predictions, skip first\n",
    "        trg = trg[1:].contiguous().view(-1)  # Flatten targets, skip <sos>\n",
    "        \n",
    "        # Calculate cross-entropy loss between predictions and true targets\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backpropagation and gradient clipping\n",
    "        loss.backward()  # Compute gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # Prevent gradient explosion\n",
    "        \n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        train_iterator.set_postfix(loss=loss.item())\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)  # Return average loss per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9301aa4-4346-42ee-b440-0bdb8f536632",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a7a647c-171d-4651-9ff6-fe4a80d506d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define the function to evaluate the model\n",
    "'''\n",
    "\n",
    "def evaluate(model, iterator, criterion): #takes model, iterator and criterion as arguments with evaluation data provided by iterator\n",
    "\n",
    "    model.eval() #Set the model to evaluation mode\n",
    "    epoch_loss = 0 #initialize epoch_loss to track accumulated loss during eval\n",
    "\n",
    "    # Wrap iterator with tqdm for progress logging\n",
    "    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n",
    "\n",
    "    with torch.no_grad(): #torch.no_grad() block ensures that no gradients computed during evaluation\n",
    "\n",
    "        for i, (src,trg) in enumerate(iterator):\n",
    "\n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            #Call the model with model(src, trg, 0) to obtain predictions for the target sequences\n",
    "            output = model(src, trg, 0) #turn off teacher forcing \n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "\n",
    "            trg = trg[1:].contiguous().view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg) #The loss between reshaped output and trg tensors is calculated with criterion\n",
    "            # Update tqdm progress bar with the current loss\n",
    "            valid_iterator.set_postfix(loss=loss.item())\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(list(iterator)) #return average loss per batch for the evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1815d7b-3b37-4f5a-91dc-bbc92d97f7d1",
   "metadata": {},
   "source": [
    "# Data Preparation and Processing from Multi30K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08a35c53-1ecd-46ae-87ab-f4a23dc1ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from multidict<7.0,>=4.5->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.9)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/christopherleggett/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.system(f\"{sys.executable} -m pip install datasets\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3358216-57b1-4c06-a929-60f5f7e69871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 29000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1014\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['en', 'de'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#Load the Multi30K Dataset with English to German Sentence Pairs\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c0eef7f-12e2-46b3-87fb-c24755af7dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Multi30K dataset...\n",
      "Dataset loaded successfully!\n",
      "Training samples: 29000\n",
      "Validation samples: 1014\n",
      "Test samples: 1000\n",
      "Building vocabulary for de...\n",
      "  Vocabulary size for de: 19214\n",
      "Building vocabulary for en...\n",
      "  Vocabulary size for en: 10837\n",
      "\n",
      "Creating DataLoaders...\n",
      "\n",
      "Train DataLoader: 3625 batches\n",
      "Valid DataLoader: 126 batches\n",
      "\n",
      "Batch shapes:\n",
      "  Source: torch.Size([6, 8]) (seq_len, batch_size)\n",
      "  Target: torch.Size([11, 8]) (seq_len, batch_size)\n",
      "\n",
      "Sample translation pair:\n",
      "  German: <bos> @@ <eos>\n",
      "  English: <bos> Front stroke swimming race roped off lap areas . <eos>\n"
     ]
    }
   ],
   "source": [
    "%run Multi30KDataLoader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "811d9d29-a752-4bcb-a826-674a79f7fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German vocabulary size: 19214\n",
      "English vocabulary size: 10837\n"
     ]
    }
   ],
   "source": [
    "#Sanity checks\n",
    "# Check vocabulary sizes\n",
    "print(f\"German vocabulary size: {len(vocab_transform['de'])}\")\n",
    "print(f\"English vocabulary size: {len(vocab_transform['en'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25940953-1a5f-45d9-b53b-84b7f303d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7aede2f-4cc5-42d8-8bf0-5544efd77e39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,     2,     2,     2],\n",
       "         [ 5510,  5510, 12642,    84],\n",
       "         [    3,     3,     8,  2356],\n",
       "         [    1,     1,  1701,     4],\n",
       "         [    1,     1,     3,     3]]),\n",
       " tensor([[   2,    2,    2,    2],\n",
       "         [6650,  216,    6,   83],\n",
       "         [4623,  110, 3398,   82],\n",
       "         [ 259, 3913,  202,    7],\n",
       "         [ 172, 1650,  109,    4],\n",
       "         [9953, 3823,   37,  708],\n",
       "         [ 115,   71,    3,    5],\n",
       "         [ 692, 2808,    1,    3],\n",
       "         [3428, 2187,    1,    1],\n",
       "         [   5,    5,    1,    1],\n",
       "         [   3,    3,    1,    1]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src, trg = next(iter(train_dataloader)) #Check the src and trg tensors\n",
    "src,trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2412d9ac-c8b8-4d21-8873-9d9c24f531a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________\n",
      "german\n",
      "<bos> Eine Gruppe Menschen protestiert in einer Stadt . <eos>\n",
      "<bos> Eine Gruppe teilt ihre politischen Ansichten mit . <eos>\n",
      "<bos> Mehrere Personen sitzen an einem felsigen Strand . <eos>\n",
      "<bos> Zwei sitzende Personen mit Hüten und Sonnenbrillen . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> A group of people protesting in a city . <eos> <pad> <pad>\n",
      "<bos> A group is letting their political opinion be known . <eos> <pad>\n",
      "<bos> A group of people are sitting on a rocky beach . <eos>\n",
      "<bos> Two people sitting in hats and shades . <eos> <pad> <pad> <pad>\n",
      "________________\n",
      "german\n",
      "<bos> Ein kleiner Junge mit Hut beim Angeln . <eos>\n",
      "<bos> Diese zwei Frauen haben Spaß im Giorgio's . <eos>\n",
      "<bos> Zwei kleine Kinder schlafen auf dem Sofa . <eos>\n",
      "<bos> Zwei junge Mädchen marschieren in einem Umzug . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> A young boy in a hat is fishing by himself . <eos>\n",
      "<bos> These two women is at Giorgio 's having fun . <eos> <pad>\n",
      "<bos> Two young children are asleep on a couch . <eos> <pad> <pad>\n",
      "<bos> Two young girls walk in a parade . <eos> <pad> <pad> <pad>\n",
      "________________\n",
      "german\n",
      "<bos> Eine Frau läuft vor einer gestreiften Wand . <eos>\n",
      "<bos> Ein Mann fährt Jet-Ski auf dem Ozean . <eos>\n",
      "<bos> Die städtischen Straßenbahnen an einem sonnigen Tag . <eos>\n",
      "<bos> Die Vorderseite eines Schnellimbiss in einem Park . <eos>\n",
      "________________\n",
      "english\n",
      "<bos> A woman is running in front of a striped wall . <eos> <pad>\n",
      "<bos> A man rides a jet ski across the ocean . <eos> <pad> <pad>\n",
      "<bos> The urban trolly 's of a city on a sunny day . <eos>\n",
      "<bos> The front side of a food vendor in a park . <eos> <pad>\n"
     ]
    }
   ],
   "source": [
    "data_itr = iter(train_dataloader)\n",
    "# moving forward in the dataset to reach sequences of longer length for illustration purpose.\n",
    "for n in range(1000):\n",
    "    german, english= next(data_itr)\n",
    "\n",
    "for n in range(3):\n",
    "    german, english=next(data_itr)\n",
    "    german=german.T\n",
    "    english=english.T\n",
    "    print(\"________________\")\n",
    "    print(\"german\")\n",
    "    for g in german:\n",
    "        print(index_to_german(g))\n",
    "    print(\"________________\")\n",
    "    print(\"english\")\n",
    "    for e in english:\n",
    "        print(index_to_eng(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8407e0dc-1e7e-419e-b7cd-6bb96bbe0614",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "282863b7-c67c-45a9-8380-c95e26be34b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Sets random seed for various libraries and modules. This is done to make the results reproducible:\n",
    "\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de5f4a6d-84ad-4f1f-903c-21e61015376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform['de'])\n",
    "OUTPUT_DIM = len(vocab_transform['en'])\n",
    "ENC_EMB_DIM = 128 #256\n",
    "DEC_EMB_DIM = 128 #256\n",
    "HID_DIM = 256 #512\n",
    "N_LAYERS = 1 #2\n",
    "ENC_DROPOUT = 0.3 #0.5\n",
    "DEC_DROPOUT = 0.3 #0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "996fb4bd-536c-4ec3-a5f6-ab1211231e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(19214, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(10837, 128)\n",
       "    (lstm): LSTM(128, 256, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=256, out_features=10837, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (trg_vocab): Vocab()\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3ba3f9f-82d9-4316-a8cf-6d76ca283f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,422,165 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f377a2ab-1c49-4760-b3bd-340d551f61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe674cc2-4bfc-43f4-abb7-d3877e83f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb6cb48-9cbb-42c5-b6c5-c44b3c1bc873",
   "metadata": {},
   "source": [
    "### What's Happening During Training?\n",
    "- Epoch loop: Running 3 complete passes through the entire 7,250-batch training dataset to iteratively improve model weights\n",
    "- Forward pass: Each batch feeds source sentences through encoder → context vectors → decoder with teacher forcing (50% true targets, 50% predictions) → output probabilities\n",
    "- Loss calculation: Cross-entropy measures how wrong predictions are compared to true target tokens, excluding <sos> position\n",
    "- Backpropagation: Computing gradients through seq2seq model using PyTorch's autograd to determine weight update directions\n",
    "- Gradient clipping: Capping gradient magnitudes at 1.0 to prevent exploding gradients common in RNNs during BPTT\n",
    "- Perplexity tracking: Exponentiating loss gives perplexity - lower values (approaching 1.0) indicate model is less \"confused\" about predictions\n",
    "- Model checkpointing: Saving weights when validation loss improves ensures you keep the best-performing version\n",
    "- Validation evaluation: After each epoch, running model on held-out data without gradients to monitor generalization\n",
    "- CPU execution: Using CPU instead of MPS due to PyTorch's incomplete Metal support for LSTM operations on Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db23149-6a12-4823-b650-4cba10fba03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Training on device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 13m 48s\n",
      "\tTrain Loss: 4.403 | Train PPL:  81.697\n",
      "\t Val. Loss: 5.205 |  Val. PPL: 182.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 1197/7250 [01:07<06:06, 16.52it/s, loss=3.75]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "# Force CPU - more reliable for RNNs on Mac\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure model is on CPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Training parameters\n",
    "N_EPOCHS = 3  # Start with 3, can increase to 5+ later\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "best_train_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_PPLs = []\n",
    "valid_PPLs = []\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train for one epoch\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    train_ppl = math.exp(train_loss)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    valid_loss = evaluate(model, valid_dataloader, criterion)\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'RNN-TR-model.pt')\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_PPLs.append(train_ppl)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_PPLs.append(valid_ppl)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
