{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c81b0b7-be40-405f-b92b-2c42bbea3cc0",
   "metadata": {},
   "source": [
    "# Next Word Prediction using N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bc2ba3-54ac-4a91-bb6b-92e48465e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import requests\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class RobustVocabulary:\n",
    "    def __init__(self, texts, min_freq=2, max_vocab_size=10000):\n",
    "        all_tokens = []\n",
    "        for text in texts:\n",
    "            all_tokens.extend(self.tokenize(text))\n",
    "        \n",
    "        self.token_counts = Counter(all_tokens)\n",
    "        common_tokens = [token for token, count in self.token_counts.most_common() \n",
    "                        if count >= min_freq][:max_vocab_size-2]\n",
    "        \n",
    "        self.vocab = ['<pad>', '<unk>'] + common_tokens\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.vocab)}\n",
    "        self.idx_to_token = {idx: token for token, idx in self.token_to_idx.items()}\n",
    "        \n",
    "        print(f\"Vocabulary: {len(self.vocab)} tokens\")\n",
    "        print(f\"Top words: {common_tokens[:10]}\")\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        text = text.lower().strip()\n",
    "        text = re.sub(r\"'\", \"'\", text)\n",
    "        tokens = re.findall(r\"\\b\\w+(?:'\\w+)?\\b\", text)\n",
    "        return tokens\n",
    "    \n",
    "    def get_itos(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    def encode(self, tokens):\n",
    "        return [self.token_to_idx.get(token, 1) for token in tokens]\n",
    "\n",
    "class NGramDataset(Dataset):\n",
    "    def __init__(self, texts, vocab, context_size=3):\n",
    "        self.vocab = vocab\n",
    "        self.context_size = context_size\n",
    "        self.data = self.create_ngram_data(texts)\n",
    "    \n",
    "    def create_ngram_data(self, texts):\n",
    "        all_data = []\n",
    "        \n",
    "        for text in texts:\n",
    "            tokens = self.vocab.tokenize(text)\n",
    "            if len(tokens) < self.context_size + 1:\n",
    "                continue\n",
    "                \n",
    "            token_indices = self.vocab.encode(tokens)\n",
    "            \n",
    "            for i in range(self.context_size, len(token_indices)):\n",
    "                context = token_indices[i-self.context_size:i]\n",
    "                target = token_indices[i]\n",
    "                all_data.append((torch.tensor(context), torch.tensor(target)))\n",
    "        \n",
    "        return all_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "class ImprovedNGramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, context_size=3, hidden_dim=256, dropout=0.3):\n",
    "        super(ImprovedNGramModel, self).__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.fc1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.fc3 = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        context_vector = embeds.view(embeds.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.bn1(self.fc1(context_vector)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        output = self.fc3(x)\n",
    "        return output\n",
    "\n",
    "def load_sample_data():\n",
    "    try:\n",
    "        print(\"Downloading data...\")\n",
    "        url = \"https://www.gutenberg.org/files/74/74-0.txt\"\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            text = response.text\n",
    "            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 20]\n",
    "            print(f\"Downloaded {len(sentences)} sentences\")\n",
    "            return sentences[:2000]\n",
    "        else:\n",
    "            raise Exception(f\"HTTP {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def train_model_with_validation(model, train_loader, val_loader, epochs=50, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for contexts, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(contexts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for contexts, targets in val_loader:\n",
    "                outputs = model(contexts)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_ngram_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}')\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_ngram_model.pth'))\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "def evaluate_model(model, test_loader, vocab):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in test_loader:\n",
    "            outputs = model(contexts)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_predictions += targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, Perplexity: {perplexity:.4f}\")\n",
    "    \n",
    "    return avg_loss, accuracy, perplexity\n",
    "\n",
    "def interactive_predict_improved(model, vocab, context_size=3):\n",
    "    print(f\"Interactive prediction (context: {context_size}, vocab: {len(vocab)})\")\n",
    "    print(\"Commands: 'sample', 'quit', or enter text\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter text: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if user_input.lower() == 'sample':\n",
    "            print(', '.join(vocab.get_itos()[2:32]))\n",
    "            continue\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            tokens = vocab.tokenize(user_input)\n",
    "            token_indices = vocab.encode(tokens)\n",
    "            \n",
    "            if len(token_indices) < context_size:\n",
    "                padding_needed = context_size - len(token_indices)\n",
    "                token_indices = [0] * padding_needed + token_indices\n",
    "            \n",
    "            context = token_indices[-context_size:]\n",
    "            context_tensor = torch.tensor([context])\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(context_tensor)\n",
    "                probabilities = F.softmax(output, dim=1).squeeze()\n",
    "                top_probs, top_indices = probabilities.topk(3)\n",
    "                \n",
    "                for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "                    word = vocab.get_itos()[idx.item()]\n",
    "                    print(f\"{i}. {word} ({prob.item()*100:.1f}%)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    CONTEXT_SIZE = 3\n",
    "    EMBEDDING_DIM = 128\n",
    "    HIDDEN_DIM = 256\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 50\n",
    "    \n",
    "    texts = load_sample_data()\n",
    "    if not texts:\n",
    "        print(\"No data available\")\n",
    "        exit()\n",
    "    \n",
    "    train_texts, temp_texts = train_test_split(texts, test_size=0.4, random_state=42)\n",
    "    val_texts, test_texts = train_test_split(temp_texts, test_size=0.5, random_state=42)\n",
    "    \n",
    "    print(f\"Train: {len(train_texts)}, Val: {len(val_texts)}, Test: {len(test_texts)}\")\n",
    "    \n",
    "    vocab = RobustVocabulary(train_texts, min_freq=2, max_vocab_size=5000)\n",
    "    \n",
    "    train_dataset = NGramDataset(train_texts, vocab, CONTEXT_SIZE)\n",
    "    val_dataset = NGramDataset(val_texts, vocab, CONTEXT_SIZE)\n",
    "    test_dataset = NGramDataset(test_texts, vocab, CONTEXT_SIZE)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = ImprovedNGramModel(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        context_size=CONTEXT_SIZE,\n",
    "        hidden_dim=HIDDEN_DIM\n",
    "    )\n",
    "    \n",
    "    print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    train_losses, val_losses = train_model_with_validation(\n",
    "        model, train_loader, val_loader, EPOCHS\n",
    "    )\n",
    "    \n",
    "    evaluate_model(model, test_loader, vocab)\n",
    "    interactive_predict_improved(model, vocab, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66282c34-48ee-4ade-9535-e9d00ae69bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple interactive prediction cell - run after training on Tom Sawyer dataset\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_next_word():\n",
    "    print(\"N-gram Next Word Prediction\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter text: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Tokenize input\n",
    "            tokens = vocab.tokenize(user_input)\n",
    "            token_indices = vocab.encode(tokens)\n",
    "            \n",
    "            # Pad if needed\n",
    "            if len(token_indices) < CONTEXT_SIZE:\n",
    "                padding_needed = CONTEXT_SIZE - len(token_indices)\n",
    "                token_indices = [0] * padding_needed + token_indices\n",
    "            \n",
    "            # Get context\n",
    "            context = token_indices[-CONTEXT_SIZE:]\n",
    "            context_tensor = torch.tensor([context])\n",
    "            \n",
    "            # Predict\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = model(context_tensor)\n",
    "                probabilities = F.softmax(output, dim=1).squeeze()\n",
    "                top_probs, top_indices = probabilities.topk(3)\n",
    "                \n",
    "                print(f\"Predictions for '{user_input}':\")\n",
    "                for i, (prob, idx) in enumerate(zip(top_probs, top_indices), 1):\n",
    "                    word = vocab.get_itos()[idx.item()]\n",
    "                    confidence = prob.item() * 100\n",
    "                    print(f\"  {i}. {word} ({confidence:.1f}%)\")\n",
    "                print()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Start prediction\n",
    "predict_next_word()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
