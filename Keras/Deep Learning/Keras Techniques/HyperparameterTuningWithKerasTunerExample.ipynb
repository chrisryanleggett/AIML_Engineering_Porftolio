{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebd2d43-7dc0-47eb-a40f-cfdb3b033312",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using Keras Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b70ae-3cd7-4758-9292-37ad5b32e33c",
   "metadata": {},
   "source": [
    "**Note on the importance of hyperparameters** The hyperparameters are the variables that govern the training process of a model such as learning rate, batch size, and number of layers of the neural network.  Keras Tuner is a useful library to help automate the proces of hyparparameter tuning which allows defining a model with hyperparameters, configure the search and the run the hyperparameter search then analyze the results and train the optimized model.\n",
    "\n",
    "- This notebook demonstrates how to use the keras tuner library to set hyperparameters (e.g. learning rate, batch size, number of layers of units per layer) before the training begins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ed16cbb-105c-4f12-81b8-673165f611d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras tuner is already installed\n"
     ]
    }
   ],
   "source": [
    "#Set up environment and import and install keras tuner if not already installed\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress urllib3 warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='urllib3')\n",
    "\n",
    "try:\n",
    "   import keras_tuner\n",
    "   print(\"keras tuner is already installed\")\n",
    "except ImportError:\n",
    "   print(\"Installing keras-tuner...\")\n",
    "   subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"keras-tuner\"])\n",
    "   print(\"Done\")\n",
    "\n",
    "#Imports Keras modules (keras tuner, tensorflow and keras components for building model & loading dataset)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras_tuner as kt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fd5be35-7fea-412f-944e-0f5653b1bc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from files_generated_from_notebooks/intro_to_kt/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore', message='Do not pass an `input_shape`')\n",
    "\n",
    "#Defines a model building function that specifies the model parameters that you want to tune\n",
    "def build_model(hp):\n",
    "   model = Sequential([\n",
    "       Flatten(input_shape=(28, 28)), \n",
    "       Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "       Dense(10, activation='softmax')\n",
    "   ])\n",
    "   model.compile(\n",
    "       optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')), \n",
    "       loss='sparse_categorical_crossentropy', \n",
    "       metrics=['accuracy'])\n",
    "   return model\n",
    "\n",
    "#configure the search process by creating a random search tuner\n",
    "tuner = kt.RandomSearch(\n",
    "   build_model, \n",
    "   objective='val_accuracy',\n",
    "   max_trials=10,\n",
    "   executions_per_trial=2,\n",
    "   directory='files_generated_from_notebooks',\n",
    "   project_name='intro_to_kt'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5750854-b9bd-492e-9f9e-21d0397ea78f",
   "metadata": {},
   "source": [
    "**Once the tuner is configured, you can run the hyperparameter search**\n",
    "- The tuner will evaluate different hyperparameter combinations and find the best based on validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "387c8539-268a-40f5-920c-3dd93ffe19da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 28s]\n",
      "val_accuracy: 0.9769000113010406\n",
      "\n",
      "Best val_accuracy So Far: 0.9796499907970428\n",
      "Total elapsed time: 00h 04m 07s\n"
     ]
    }
   ],
   "source": [
    "#Hyperparameter search\n",
    "\n",
    "#Load and pre-process the mnist dataset\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "#Run the hyperparameter search using the search method\n",
    "tuner.search(x_train, y_train, epochs=5, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c841b30-4d32-4b56-aee4-314be60b741a",
   "metadata": {},
   "source": [
    "**Last step:** After the search is complete and best values are discovered, we need to build a model with these optimized values.  Below cells demonstrate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f425594a-a2e6-4e77-b789-01f56c4dc1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal number of units in the first dense layer is 256. The optimal learning rate for the optimizer is 0.0009328604664075379.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrieve and print the best hyperparameter values using the get_best_hyperparameters \n",
    "function and print the best hyperparameters\n",
    "\"\"\"\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"The optimal number of units in the first dense layer is {best_hps.get('units')}. The optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\")\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0768ae-81a3-48b7-a8ea-2e99787778c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9971 - loss: 0.0089 - val_accuracy: 0.9777 - val_loss: 0.0956\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9974 - loss: 0.0086 - val_accuracy: 0.9795 - val_loss: 0.0946\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0059 - val_accuracy: 0.9783 - val_loss: 0.1049\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0057 - val_accuracy: 0.9774 - val_loss: 0.1062\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9991 - loss: 0.0042 - val_accuracy: 0.9783 - val_loss: 0.1033\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9982 - loss: 0.0059 - val_accuracy: 0.9785 - val_loss: 0.1091\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0056 - val_accuracy: 0.9778 - val_loss: 0.1181\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 0.9778 - val_loss: 0.1165\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9990 - loss: 0.0038 - val_accuracy: 0.9800 - val_loss: 0.1122\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9984 - loss: 0.0066 - val_accuracy: 0.9788 - val_loss: 0.1190\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 392us/step - accuracy: 0.9765 - loss: 0.1130\n",
      "Test accuracy: 0.9799000024795532\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data (if not already done)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#Train the model with the optimized hyparaparameters\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "#Evaluate the model's performance on the test set \n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087d6c1-780e-4abe-94f1-a1ccc59344d5",
   "metadata": {},
   "source": [
    "# Summary and results\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrates automated hyperparameter optimization using Keras Tuner's RandomSearch to find optimal network architecture and learning rates. The tuner systematically tested different combinations of hidden layer units (32-512) and learning rates (0.0001-0.01) to maximize validation accuracy on MNIST digit classification.\n",
    "\n",
    "## Results\n",
    "\n",
    "The automated tuning successfully found optimal hyperparameters that achieved nearly 98% test accuracy on MNIST. Signs of mild overfitting are evident with training accuracy (99.84%) significantly higher than validation accuracy (97.88%), suggesting the model memorized some training patterns. \n",
    "\n",
    "## Results interpretation\n",
    "97.99% test accuracy is excellent for a simple neural network on MNIST.  ~2% gap between training (99.84%) and validation (97.88%) is normal and manageable. MNIST is designed to be learnable, so high accuracy is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fb67c-8b8c-4521-9027-5941c5c3bf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
