{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f139319d-4672-4529-aa05-0ca1fa45f55a",
   "metadata": {},
   "source": [
    "# Model optimization examples\n",
    "\n",
    "*Model optimization* is crucial to enhance performance, efficiency and scalability of models.  This improves training times and utilization of hardware resources. \n",
    "\n",
    "Common optimization techniques include: \n",
    "- Weight initalization\n",
    "- Learning rate scheduling\n",
    "- Batch normalization\n",
    "\n",
    "Proper initialization of weights can impact the convergence and performance of a neural network.  Methods include Glorot and He initialization methods to avoid vanishing or exploding gradients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b0672b-37d4-4bbb-9353-82a2c574e42d",
   "metadata": {},
   "source": [
    "## Technique 1: Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e55efb-4aa7-489d-926d-19693a08312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress urllib3 and Keras input_shape warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='urllib3')\n",
    "warnings.filterwarnings('ignore', message='Do not pass an `input_shape`')\n",
    "\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Define the model with 'He' initialization for better weight initialization\n",
    "model = Sequential([\n",
    "   # Flatten layer: converts 28x28 image to 784-element vector\n",
    "   Flatten(input_shape=(28, 28)), \n",
    "   \n",
    "   # Hidden layer: 128 neurons with ReLU activation and He Normal initialization\n",
    "   Dense(128, activation='relu', kernel_initializer=HeNormal()), \n",
    "   \n",
    "   # Output layer: 10 neurons for digit classification (0-9)\n",
    "   Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f61b7-5d2a-493a-aadd-1c14dfef078d",
   "metadata": {},
   "source": [
    "## Technique 2: Learning rate scheduling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2c02712-0ada-4abd-9e3e-6a016294d10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and Preprocess Dataset and get it ready for training using the Mnist dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#Load MNIST dataset\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "\n",
    "#Normalize input data the pixel values to between  0 and 1 for better performance during training\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_val = x_val.astype('float32') / 255.0\n",
    "\n",
    "#Reshape input data (if necessary) to ensure it is in the correct format\n",
    "x_train = x_train.reshape(-1, 28, 28)\n",
    "x_val = x_val.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f904a8a-fd4e-4c90-8d6f-42acde1fcc7c",
   "metadata": {},
   "source": [
    "**Implement a learning rate scheduler to adjust the learning rate dynamically during training**\n",
    "*(This will help the model converge more efficiently)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8663d934-6e4b-43ea-9238-fb764d3c9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#Define a scheduler function that keeps the learning rate constant for the first ten epochs then exponentially decreases it\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "       return lr\n",
    "   else:\n",
    "       return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8b8b1e2-de51-4563-941f-463fba22d9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 676us/step - accuracy: 0.8782 - loss: 0.4371 - val_accuracy: 0.9592 - val_loss: 0.1361 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656us/step - accuracy: 0.9648 - loss: 0.1209 - val_accuracy: 0.9702 - val_loss: 0.0945 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640us/step - accuracy: 0.9773 - loss: 0.0766 - val_accuracy: 0.9739 - val_loss: 0.0880 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step - accuracy: 0.9834 - loss: 0.0562 - val_accuracy: 0.9694 - val_loss: 0.0929 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - accuracy: 0.9870 - loss: 0.0442 - val_accuracy: 0.9751 - val_loss: 0.0783 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step - accuracy: 0.9897 - loss: 0.0330 - val_accuracy: 0.9766 - val_loss: 0.0764 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 660us/step - accuracy: 0.9925 - loss: 0.0251 - val_accuracy: 0.9778 - val_loss: 0.0781 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 665us/step - accuracy: 0.9939 - loss: 0.0209 - val_accuracy: 0.9784 - val_loss: 0.0795 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 657us/step - accuracy: 0.9947 - loss: 0.0172 - val_accuracy: 0.9764 - val_loss: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 664us/step - accuracy: 0.9956 - loss: 0.0140 - val_accuracy: 0.9759 - val_loss: 0.0880 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653us/step - accuracy: 0.9968 - loss: 0.0108 - val_accuracy: 0.9770 - val_loss: 0.0914 - learning_rate: 9.0484e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 663us/step - accuracy: 0.9979 - loss: 0.0083 - val_accuracy: 0.9780 - val_loss: 0.0830 - learning_rate: 8.1873e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 640us/step - accuracy: 0.9991 - loss: 0.0047 - val_accuracy: 0.9775 - val_loss: 0.0961 - learning_rate: 7.4082e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 668us/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 0.9788 - val_loss: 0.0861 - learning_rate: 6.7032e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 652us/step - accuracy: 0.9995 - loss: 0.0029 - val_accuracy: 0.9793 - val_loss: 0.0869 - learning_rate: 6.0653e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 638us/step - accuracy: 0.9996 - loss: 0.0021 - val_accuracy: 0.9802 - val_loss: 0.0894 - learning_rate: 5.4881e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 661us/step - accuracy: 0.9999 - loss: 0.0019 - val_accuracy: 0.9810 - val_loss: 0.0850 - learning_rate: 4.9659e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 648us/step - accuracy: 1.0000 - loss: 9.6513e-04 - val_accuracy: 0.9810 - val_loss: 0.0868 - learning_rate: 4.4933e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 653us/step - accuracy: 1.0000 - loss: 8.1782e-04 - val_accuracy: 0.9805 - val_loss: 0.0954 - learning_rate: 4.0657e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 672us/step - accuracy: 1.0000 - loss: 5.8922e-04 - val_accuracy: 0.9799 - val_loss: 0.0914 - learning_rate: 3.6788e-04\n"
     ]
    }
   ],
   "source": [
    "#Model evaluation\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0, 1] range\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding) for categorical_crossentropy\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_val = to_categorical(y_val, 10)\n",
    "\n",
    "# Compile the model with Adam optimizer and categorical crossentropy loss\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model for 20 epochs with learning rate scheduler to improve convergence\n",
    "# Train the model's performance on unseen validation data\n",
    "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=20, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f172cef-621f-4bb3-acd1-3c277ed7e57a",
   "metadata": {},
   "source": [
    "## Technique 3: Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d9770f-ecac-4a57-a915-15541ab30b0c",
   "metadata": {},
   "source": [
    "Additional techniques exist like batch normalization, mixed precision training, model pruning and quantization. \n",
    "\n",
    "1. Batch normalization: can normalize the input layer by adjusting and scaling the activations\n",
    "2. Mixed precision training: Uses 16-bit and 32-bit floating point to speed up training on modern GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "855868c7-fed8-4a37-a0ee-184367fd6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for batch normalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, BatchNormalization\n",
    "\n",
    "# Define a simple model with Batch Normalization for improved training stability\n",
    "model = Sequential([\n",
    "   Flatten(input_shape=(28, 28)), # Input layer: flatten 28x28 images to 784-element vectors\n",
    "   Dense(128, activation='relu'), # Hidden layer: 128 neurons with ReLU activation\n",
    "   BatchNormalization(), # Batch normalization: normalizes inputs to next layer for faster convergence\n",
    "   Dense(10, activation='softmax') # Output layer: 10 neurons for digit classification\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "293d4154-cc3d-4ea6-800c-0cb413643e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 855us/step - accuracy: 0.8814 - loss: 0.4288 - val_accuracy: 0.9601 - val_loss: 0.1391\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 807us/step - accuracy: 0.9657 - loss: 0.1157 - val_accuracy: 0.9685 - val_loss: 0.1037\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 816us/step - accuracy: 0.9769 - loss: 0.0802 - val_accuracy: 0.9755 - val_loss: 0.0825\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 861us/step - accuracy: 0.9822 - loss: 0.0584 - val_accuracy: 0.9764 - val_loss: 0.0778\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 855us/step - accuracy: 0.9875 - loss: 0.0427 - val_accuracy: 0.9762 - val_loss: 0.0762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x30fb25e20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for mixed precision training to improve performance and reduce memory usage\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Enable mixed precision policy for faster training\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the data to [0, 1] range\n",
    "x_train, x_test = x_train.astype('float32') / 255.0, x_test.astype('float32') / 255.0\n",
    "\n",
    "# Define a simple model with mixed precision\n",
    "model = models.Sequential([\n",
    "   layers.Input(shape=(28, 28)), \n",
    "   layers.Flatten(),\n",
    "   layers.Dense(128, activation='relu'),\n",
    "   layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with optimizer and loss function\n",
    "optimizer = optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3175006-4dbb-4fed-8105-da4333284015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d8e3e-41e5-4d24-bd75-57e65d953fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
