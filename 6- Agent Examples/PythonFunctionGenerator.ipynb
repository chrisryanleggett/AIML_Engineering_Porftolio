{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230de3ca-1299-4bbd-a1f4-d7b1b8e90d6b",
   "metadata": {},
   "source": [
    "# Python Function Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7696d698-9b56-4711-967b-9517f4f37f20",
   "metadata": {},
   "source": [
    "* This notebook demonstrates prompt-engineering techniques and practical prompt-engineering code snippets needed for building agent-like behavior:\n",
    "    * **Prompt chaining** - Breaking complex tasks into sequential steps\n",
    "    * **Context management** - Controlling what the LLM \"remembers\"\n",
    "    * **Role assignment** - Using system messages to set behavior\n",
    "    * **Output formatting** - Requesting specific formats (code blocks)\n",
    "    * **Memory manipulation** - Editing conversation history to shape future responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65ae9b4-2072-4f12-be82-76ac829ef8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ .env file loaded successfully\n",
      "✓ API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# FIRST TIME SETUP (run these commands in terminal, not in notebook):\n",
    "# 1. Create virtual environment: python3 -m venv venv\n",
    "# 2. Activate it: source venv/bin/activate  (macOS/Linux) or venv\\Scripts\\activate (Windows)\n",
    "# 3. Install dependencies: pip install -r requirements.txt\n",
    "# 4. Create .env file with: OPENAI_API_KEY=sk-your_key_here\n",
    "# 5. Start Jupyter: jupyter notebook\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "env_loaded = load_dotenv()\n",
    "\n",
    "if env_loaded:\n",
    "    print(\"✓ .env file loaded successfully\")\n",
    "else:\n",
    "    print(\"✗ .env file not found\")\n",
    "\n",
    "# Get the API key from environment\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# Validate key exists and has correct format\n",
    "if api_key and api_key.startswith('sk-'):\n",
    "    print(\"✓ API key loaded successfully\")\n",
    "else:\n",
    "    print(\"✗ API key not found or invalid format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef1f0f8-6140-4a33-98f7-40b152fcddd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c1ab04d-e917-4109-b72f-2a9ee8042b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter a description of what kind of function would you like to create?\n",
      "Enter your description: "
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " A function that prints a sum of the total number of tokens consumed by the most recent api call (should print a real number and not an estimate)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Function:\n",
      "==================================================\n",
      "\n",
      "REQUIRED MODIFICATIONS:\n",
      "from litellm import completion, get_usage\n",
      "\n",
      "NEW FUNCTION:\n",
      "def print_last_api_call_tokens():\n",
      "    \"\"\"Prints the total number of tokens consumed by the most recent API call.\"\"\"\n",
      "    usage = get_usage()\n",
      "    total_tokens = usage['total_tokens']\n",
      "    print(total_tokens)\n",
      "\n",
      "==================================================\n",
      "NOTE: Check if existing code needs updates\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "Documentation:\n",
      "==================================================\n",
      "\n",
      "def print_last_api_call_tokens():\n",
      "    \"\"\"\n",
      "    Prints the total number of tokens consumed by the most recent API call.\n",
      "\n",
      "    Parameters: \n",
      "        None\n",
      "\n",
      "    Returns:\n",
      "        None\n",
      "\n",
      "    Example:\n",
      "        print_last_api_call_tokens()  # Outputs the total token count from the last API call\n",
      "    \"\"\"\n",
      "\n",
      "==================================================\n",
      "Test Cases:\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "import unittest\n",
      "from unittest.mock import patch\n",
      "\n",
      "class TestPrintLastApiCallTokens(unittest.TestCase):\n",
      "\n",
      "    @patch('litellm.get_usage', return_value={'total_tokens': 150})\n",
      "    def test_correct_token_output(self, mock_get_usage):\n",
      "        # Tests if function prints correct token count\n",
      "        with patch('builtins.print') as mock_print:\n",
      "            print_last_api_call_tokens()\n",
      "            mock_print.assert_called_once_with(150)\n",
      "\n",
      "    @patch('litellm.get_usage', return_value={'total_tokens': 0})\n",
      "    def test_zero_token_output(self, mock_get_usage):\n",
      "        # Tests if function handles zero token count\n",
      "        with patch('builtins.print') as mock_print:\n",
      "            print_last_api_call_tokens()\n",
      "            mock_print.assert_called_once_with(0)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n"
     ]
    }
   ],
   "source": [
    "# Write a function to handle LLM interactions\n",
    "def generate_response(messages: List[Dict]) -> str:\n",
    "    \"\"\"Call LLM to get response\"\"\"\n",
    "    response = completion(\n",
    "        model=\"openai/gpt-4o\",\n",
    "        messages=messages,\n",
    "        max_tokens=1024\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Define a function that extracts code returned by response and returns it as a string\n",
    "def extract_code_block(response: str) -> str:\n",
    "    \"\"\"Extract clean code from LLM response\"\"\"\n",
    "    # If response does not contain code block markers, return response as is\n",
    "    if not '```' in response:\n",
    "        return response\n",
    "    \n",
    "    # Extract code between first pair of triple backticks\n",
    "    code_block = response.split('```')[1].strip()\n",
    "    \n",
    "    # LLMs often return markdown with 'python' at the start of the returned code. If present, remove it\n",
    "    if code_block.startswith(\"python\"):\n",
    "        code_block = code_block[6:]\n",
    "    \n",
    "    return code_block\n",
    "\n",
    "def develop_custom_function():\n",
    "    # Get all executed notebook code as context\n",
    "    from IPython import get_ipython\n",
    "    ipython = get_ipython()\n",
    "    notebook_context = '\\n\\n'.join([code for _, _, code in ipython.history_manager.get_range(output=False)])\n",
    "    \n",
    "    # Ask the user what kind of function they would like to create\n",
    "    print(\"\\nEnter a description of what kind of function would you like to create?\")\n",
    "    print(\"Enter your description: \", end='')\n",
    "    function_description = input().strip()\n",
    "    \n",
    "    # Initialize conversation with system prompt - this sets the LLM's role/personality\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a Python expert. Be concise and professional. Output ONLY code with minimal comments.\"}\n",
    "    ]\n",
    "    \n",
    "    # ===== FIRST PROMPT: Write the basic function =====\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"\"\"Here is the code executed so far in this notebook:\n",
    "```python\n",
    "{notebook_context}\n",
    "```\n",
    "\n",
    "Write a Python function that {function_description}. Make it compatible with the existing code above. This function will be called in the next cell and must run successfully without errors. \n",
    "\n",
    "IMPORTANT: If any existing code needs to be modified for this function to work (e.g., function signatures, return values, global variables), output those modifications FIRST in a separate code block labeled \"REQUIRED MODIFICATIONS:\", followed by the new function definition.\n",
    "\n",
    "Output in ```python code block``` format. No examples, no usage code.\"\"\"\n",
    "    })\n",
    "    \n",
    "    initial_function = generate_response(messages)\n",
    "    initial_function = extract_code_block(initial_function)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"```python\\n\\n\"+initial_function+\"\\n\\n```\"})\n",
    "    \n",
    "    # ===== SECOND PROMPT: Add documentation =====\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Add concise documentation to this function (6 sentences maximum). Include: brief description, parameters, returns, and one example. Output in a ```python code block```.\"\n",
    "    })\n",
    "    \n",
    "    documentation = generate_response(messages)\n",
    "    documentation = extract_code_block(documentation)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": \"```python\\n\\n\"+documentation+\"\\n\\n```\"})\n",
    "    \n",
    "    # ===== THIRD PROMPT: Create test cases =====\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Add unittest test cases. Include ONE single-line comment per test explaining what it tests. No multi-line comments, no docstrings in tests, no example usage. Output in a ```python code block```.\"\n",
    "    })\n",
    "    \n",
    "    test_cases = generate_response(messages)\n",
    "    test_cases = extract_code_block(test_cases)\n",
    "    \n",
    "    # Print output\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Function:\")\n",
    "    print(\"=\"*50)\n",
    "    print(initial_function)\n",
    "    \n",
    "    if \"REQUIRED MODIFICATIONS\" in initial_function or \"modify\" in initial_function.lower():\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"NOTE: Check if existing code needs updates\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Documentation:\")\n",
    "    print(\"=\"*50)\n",
    "    print(documentation)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Test Cases:\")\n",
    "    print(\"=\"*50)\n",
    "    print(test_cases)\n",
    "    \n",
    "    return documentation, test_cases, messages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    function_code, tests, messages = develop_custom_function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
